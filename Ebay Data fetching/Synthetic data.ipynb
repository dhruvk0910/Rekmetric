{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Synthetic Data Production\n",
    "\n",
    "This first code cell produces different csvs for that you can merge if you want or go to the second code cell which makes an entirely full connected dataset as a single csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fully connected datasets generated and saved!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import random\n",
    "from faker import Faker\n",
    "\n",
    "fake = Faker()\n",
    "\n",
    "# Define categories and sample brands\n",
    "categories = [\"Electronics\", \"Clothing\", \"Home\", \"Books\", \"Toys\"]\n",
    "brands = {\n",
    "    \"Electronics\": [\"Apple\", \"Samsung\", \"Sony\", \"Dell\", \"HP\"],\n",
    "    \"Clothing\": [\"Nike\", \"Adidas\", \"Levi's\", \"Gucci\", \"Puma\"],\n",
    "    \"Home\": [\"IKEA\", \"Philips\", \"Hamilton Beach\", \"Dyson\", \"Bosch\"],\n",
    "    \"Books\": [\"Penguin\", \"HarperCollins\", \"Simon & Schuster\", \"Oxford\", \"Pearson\"],\n",
    "    \"Toys\": [\"LEGO\", \"Hasbro\", \"Mattel\", \"Funko\", \"Nintendo\"]\n",
    "}\n",
    "\n",
    "# Generate Product Metadata\n",
    "def generate_product_data(num_entries=100000):\n",
    "    data = []\n",
    "    for i in range(1, num_entries + 1):\n",
    "        category = random.choice(categories)\n",
    "        product_id = f\"P{i:06d}\"\n",
    "        title = f\"{fake.word().capitalize()} {category}\"\n",
    "        brand = random.choice(brands[category])\n",
    "        price = round(random.uniform(5, 2000), 2)\n",
    "        ratings = round(random.uniform(1, 5), 1)\n",
    "        stock = random.randint(0, 100)\n",
    "        description = fake.sentence(nb_words=10)\n",
    "        keywords = \", \".join(fake.words(nb=5))\n",
    "\n",
    "        data.append([product_id, title, category, brand, price, ratings, stock, description, keywords])\n",
    "\n",
    "    return pd.DataFrame(data, columns=[\n",
    "        \"Product_ID\", \"Title\", \"Category\", \"Brand\", \"Price\", \"Ratings\", \"Stock\", \"Description\", \"Keywords\"\n",
    "    ])\n",
    "\n",
    "# Generate User Data, Purchase History, and Additional Interactions\n",
    "def generate_user_data(num_users=50000, num_purchases=200000):\n",
    "    users = []\n",
    "    purchases = []\n",
    "    reviews = []\n",
    "    bids = []\n",
    "    browsing = []\n",
    "    wishlist = []\n",
    "\n",
    "    for i in range(1, num_users + 1):\n",
    "        user_id = f\"U{i:05d}\"\n",
    "        name = fake.name()\n",
    "        email = fake.email()\n",
    "        location = fake.city()\n",
    "        users.append([user_id, name, email, location])\n",
    "\n",
    "    for _ in range(num_purchases):\n",
    "        user_id = f\"U{random.randint(1, num_users):05d}\"\n",
    "        product_id = f\"P{random.randint(1, 100000):06d}\"\n",
    "        purchase_date = fake.date_this_year()\n",
    "        purchases.append([user_id, product_id, purchase_date])\n",
    "\n",
    "        if random.random() > 0.5:  # 50% chance of leaving a review\n",
    "            review_text = fake.sentence(nb_words=15)\n",
    "            rating = round(random.uniform(1, 5), 1)\n",
    "            reviews.append([user_id, product_id, rating, review_text])\n",
    "\n",
    "        if random.random() > 0.3:  # 30% chance of bidding\n",
    "            bid_amount = round(random.uniform(10, 2000), 2)\n",
    "            bid_date = fake.date_this_year()\n",
    "            bids.append([user_id, product_id, bid_amount, bid_date])\n",
    "\n",
    "        if random.random() > 0.4:  # 40% chance of adding to wishlist\n",
    "            wishlist.append([user_id, product_id])\n",
    "\n",
    "        if random.random() > 0.6:  # 60% chance of viewing product\n",
    "            browsing.append([user_id, product_id, fake.date_this_year()])\n",
    "\n",
    "    user_df = pd.DataFrame(users, columns=[\"User_ID\", \"Name\", \"Email\", \"Location\"])\n",
    "    purchase_df = pd.DataFrame(purchases, columns=[\"User_ID\", \"Product_ID\", \"Purchase_Date\"])\n",
    "    review_df = pd.DataFrame(reviews, columns=[\"User_ID\", \"Product_ID\", \"Rating\", \"Review\"])\n",
    "    bid_df = pd.DataFrame(bids, columns=[\"User_ID\", \"Product_ID\", \"Bid_Amount\", \"Bid_Date\"])\n",
    "    wishlist_df = pd.DataFrame(wishlist, columns=[\"User_ID\", \"Product_ID\"])\n",
    "    browsing_df = pd.DataFrame(browsing, columns=[\"User_ID\", \"Product_ID\", \"View_Date\"])\n",
    "\n",
    "    return user_df, purchase_df, review_df, bid_df, wishlist_df, browsing_df\n",
    "\n",
    "# Generate and save large datasets\n",
    "product_df = generate_product_data(100000)\n",
    "user_df, purchase_df, review_df, bid_df, wishlist_df, browsing_df = generate_user_data(50000, 200000)\n",
    "\n",
    "product_df.to_csv(\"synthetic_product_data_large.csv\", index=False)\n",
    "user_df.to_csv(\"synthetic_user_data_large.csv\", index=False)\n",
    "purchase_df.to_csv(\"synthetic_purchase_data_large.csv\", index=False)\n",
    "review_df.to_csv(\"synthetic_reviews_large.csv\", index=False)\n",
    "bid_df.to_csv(\"synthetic_bids_large.csv\", index=False)\n",
    "wishlist_df.to_csv(\"synthetic_wishlist_large.csv\", index=False)\n",
    "browsing_df.to_csv(\"synthetic_browsing_large.csv\", index=False)\n",
    "\n",
    "print(\"Fully connected datasets generated and saved!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fully connected eBay-like dataset generated and saved!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import random\n",
    "from faker import Faker\n",
    "\n",
    "fake = Faker()\n",
    "\n",
    "# Define categories and sample brands\n",
    "categories = [\"Electronics\", \"Clothing\", \"Home\", \"Books\", \"Toys\"]\n",
    "brands = {\n",
    "    \"Electronics\": [\"Apple\", \"Samsung\", \"Sony\", \"Dell\", \"HP\"],\n",
    "    \"Clothing\": [\"Nike\", \"Adidas\", \"Levi's\", \"Gucci\", \"Puma\"],\n",
    "    \"Home\": [\"IKEA\", \"Philips\", \"Hamilton Beach\", \"Dyson\", \"Bosch\"],\n",
    "    \"Books\": [\"Penguin\", \"HarperCollins\", \"Simon & Schuster\", \"Oxford\", \"Pearson\"],\n",
    "    \"Toys\": [\"LEGO\", \"Hasbro\", \"Mattel\", \"Funko\", \"Nintendo\"]\n",
    "}\n",
    "\n",
    "# Generate synthetic dataset with all features in one CSV\n",
    "def generate_full_dataset(num_users=50000, num_products=100000, num_interactions=200000):\n",
    "    data = []\n",
    "    \n",
    "    for _ in range(num_interactions):\n",
    "        user_id = f\"U{random.randint(1, num_users):05d}\"\n",
    "        product_id = f\"P{random.randint(1, num_products):06d}\"\n",
    "        \n",
    "        category = random.choice(categories)\n",
    "        brand = random.choice(brands[category])\n",
    "        title = f\"{fake.word().capitalize()} {category}\"\n",
    "        price = round(random.uniform(5, 2000), 2)\n",
    "        ratings = round(random.uniform(1, 5), 1)\n",
    "        stock = random.randint(0, 100)\n",
    "        description = fake.sentence(nb_words=10)\n",
    "        keywords = \", \".join(fake.words(nb=5))\n",
    "        \n",
    "        purchase_date = fake.date_this_year() if random.random() > 0.5 else None\n",
    "        review_text = fake.sentence(nb_words=15) if random.random() > 0.5 else None\n",
    "        rating = round(random.uniform(1, 5), 1) if review_text else None\n",
    "        bid_amount = round(random.uniform(10, 2000), 2) if random.random() > 0.3 else None\n",
    "        bid_date = fake.date_this_year() if bid_amount else None\n",
    "        view_date = fake.date_this_year() if random.random() > 0.6 else None\n",
    "        wishlist_added = random.choice([True, False]) if random.random() > 0.4 else None\n",
    "        \n",
    "        data.append([user_id, product_id, title, category, brand, price, ratings, stock, description, keywords,\n",
    "                     purchase_date, review_text, rating, bid_amount, bid_date, view_date, wishlist_added])\n",
    "    \n",
    "    columns = [\"User_ID\", \"Product_ID\", \"Title\", \"Category\", \"Brand\", \"Price\", \"Ratings\", \"Stock\", \"Description\", \"Keywords\",\n",
    "               \"Purchase_Date\", \"Review\", \"Review_Rating\", \"Bid_Amount\", \"Bid_Date\", \"View_Date\", \"Wishlist_Added\"]\n",
    "    \n",
    "    return pd.DataFrame(data, columns=columns)\n",
    "\n",
    "# Generate the dataset\n",
    "dataset = generate_full_dataset()\n",
    "\n",
    "# Save to CSV\n",
    "dataset.to_csv(\"synthetic_ebay_like_dataset.csv\", index=False)\n",
    "\n",
    "print(\"Fully connected eBay-like dataset generated and saved!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
